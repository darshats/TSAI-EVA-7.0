{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Session 2 - Assignment MNIST.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPG438UULc5ImindUYGTfUc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e24c3779ed664b888548781fc6866439":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f426261738f4400b9018ce0698e5d3f3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8d41a8b67607441f89e18b29c0dd6042","IPY_MODEL_993e02a55f394e769d58835373bbe3ee","IPY_MODEL_906c910d94f7490093ac702a1333b159"]}},"f426261738f4400b9018ce0698e5d3f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8d41a8b67607441f89e18b29c0dd6042":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_47e2cf101ca64522917415e8cf1eb413","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9b187ba28e2a4aa0abbf79a66ef644c2"}},"993e02a55f394e769d58835373bbe3ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ae0753feddee4a9f9f8be91b754f6cff","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd2d3033ba864e7c91ca4eab420a560d"}},"906c910d94f7490093ac702a1333b159":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_273b342de54b46de885dce7ccd374437","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:00&lt;00:00, 48657502.05it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae88cdc9f7fd46d1abba56f33f666211"}},"47e2cf101ca64522917415e8cf1eb413":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9b187ba28e2a4aa0abbf79a66ef644c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae0753feddee4a9f9f8be91b754f6cff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fd2d3033ba864e7c91ca4eab420a560d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"273b342de54b46de885dce7ccd374437":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ae88cdc9f7fd46d1abba56f33f666211":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"h5os6fg3js1u"},"source":["# Mount/Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bUQqpSH302z","executionInfo":{"status":"ok","timestamp":1634009066441,"user_tz":-120,"elapsed":19817,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"54219084-9d7f-45b2-914d-ca35dfe8c877"},"source":["# Mounting to google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGvqYdab37qt","executionInfo":{"status":"ok","timestamp":1634009066442,"user_tz":-120,"elapsed":20,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"8626dfb1-3b85-41cd-dde9-1413f3780d32"},"source":["# Changing directory to Session 2 project folder\n","% cd /content/gdrive/My Drive/EVA 7/Session 2/"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/gdrive/My Drive/EVA 7/Session 2/'\n","/content\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBJc8Sb_4IRN","executionInfo":{"status":"ok","timestamp":1634009066443,"user_tz":-120,"elapsed":14,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"20ac092f-c2b3-4095-856b-a31fd4c75c01"},"source":["# Lists the contents of this folder\n","! ls "],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["gdrive\tsample_data\n"]}]},{"cell_type":"code","metadata":{"id":"afzQTgkz4kYu","executionInfo":{"status":"ok","timestamp":1634009091533,"user_tz":-120,"elapsed":25097,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["# Imports \n","import numpy as np \n","import matplotlib.pyplot as plt\n","\n","\"\"\"\n","Torch is an open-source machine learning library, a scientific computing framework, \n","and a script language based on the Lua programming language. \n","It provides a wide range of algorithms for deep learning.\n","\"\"\"\n","\n","import torch                                                                    \n","import torchvision                                                              # provide access to datasets, models, transforms, utils, etc\n","import torchvision.transforms as transforms                                     # for image transforms\n","\n","# linewidth – The number of characters per line for the purpose of inserting line breaks (default = 80)\n","torch.set_printoptions(linewidth=120)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zTrtMQxfp5Fi"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"UO8TfZhQV9wY","colab":{"base_uri":"https://localhost:8080/","height":471,"referenced_widgets":["e24c3779ed664b888548781fc6866439","f426261738f4400b9018ce0698e5d3f3","8d41a8b67607441f89e18b29c0dd6042","993e02a55f394e769d58835373bbe3ee","906c910d94f7490093ac702a1333b159","47e2cf101ca64522917415e8cf1eb413","9b187ba28e2a4aa0abbf79a66ef644c2","ae0753feddee4a9f9f8be91b754f6cff","fd2d3033ba864e7c91ca4eab420a560d","273b342de54b46de885dce7ccd374437","ae88cdc9f7fd46d1abba56f33f666211"]},"executionInfo":{"status":"ok","timestamp":1634009093237,"user_tz":-120,"elapsed":1787,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"b9447b5e-5f87-4b38-cd91-ef3c3dfc5d66"},"source":["from torchvision import datasets                                                # Access to benchmarking datasets\n","\n","train_set = datasets.MNIST('./data', train=True, download=True,                 # Downloading the training dataset in ./data folder\n","                    transform=transforms.Compose([                              # and transforming the images to tensors and normalizing with\n","                        transforms.ToTensor(),                                  # mean and std of the entire dataset\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ]))\n","\n","test_set = datasets.MNIST('./data', train=False, download=True,                 # Downloading the test dataset in ./data folder \n","                    transform=transforms.Compose([                              # and transforming the images to tensors and normalizing with\n","                        transforms.ToTensor(),                                  # mean and std of the entire dataset\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ]))\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e24c3779ed664b888548781fc6866439","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3685d926a3114a34add3aaf0cae7043b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc054918ec964a8db758f6cf2c044eed","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b17dd0fe408249409cb14aaa29e9200b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"]}]},{"cell_type":"code","metadata":{"id":"zBEnD0m6bjfu","executionInfo":{"status":"ok","timestamp":1634009093240,"user_tz":-120,"elapsed":103,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["def num_to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return np.eye(num_classes, dtype='uint8')[y]\n","\n","def categorical_to_num(y):\n","  \"\"\" Decodes a 1-hot encoding \"\"\"\n","  return np.argmax(y)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBYxEG9cR-f9","executionInfo":{"status":"ok","timestamp":1634009093242,"user_tz":-120,"elapsed":99,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["# Dataset is there to be able to interact with DataLoader\n","from torch.utils.data import Dataset                                            \n","\n","class mnist_data(Dataset):\n","  '''\n","  mnist_data class to create an iteratable on our custom mnist dataset:\n","  where data = (image, random number) and labels = (image number, sum of random number and image number)\n","  Attributes:\n","    mnist: mnist data object\n","  Methods:\n","    len: returns length of the dataset \n","    getitem: custom getitem method returns data and labels\n","  '''\n","  def __init__(self, d):\n","    self.mnist = d                                                              # Initialize with mnist data object\n","\n","  def __getitem__(self, index):                                                 \n","    image, label = self.mnist[index]                                              \n","    n = np.random.randint(low=0, high=10)                                       # generate a random number from 0 to 9 using randint              \n","    s = n+label                                                                 # sum of the image number and the random number n\n","    number = torch.tensor(num_to_categorical(n, 10))                            # 1-hot encoding of random number   \n","    return (image, number), (torch.tensor(label), torch.tensor(s))              # (input), (output) to network      \n","\n","  def __len__(self):                                                            # Return the length of the dataset\n","    return len(self.mnist)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vS-2SZ4Y1y7","executionInfo":{"status":"ok","timestamp":1634009093248,"user_tz":-120,"elapsed":99,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["train_set_custom = mnist_data(train_set)                                        # custom train_set from mnist_data class\n","test_set_custom = mnist_data(test_set)                                          # custom test_set from mnist_data class"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwY7tLYDaAhc","executionInfo":{"status":"ok","timestamp":1634009093250,"user_tz":-120,"elapsed":98,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["use_cuda = torch.cuda.is_available()                                            # use gpu if available\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")                            # assigning gpu to variable device\n","\n","# If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, \n","# you can speed up the host to device transfer by enabling pin_memory.\n","# num_workers attribute tells the data loader instance how many sub-processes to use for data loading\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}             \n","batch_size = 100                                                                # set size of batch                                    \n","\n","train_loader = torch.utils.data.DataLoader(                                     # For iterating through our dataset in batches      \n","    train_set_custom,\n","    batch_size = batch_size,  \n","    shuffle=True,                                                               # Shuffle the data points randomly\n","    **kwargs\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_set_custom,\n","    batch_size = batch_size,\n","    shuffle=True, \n","    **kwargs\n",")"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yz8PV5Jup2BY"},"source":["# Visualization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"rHsdNGtyZBws","executionInfo":{"status":"ok","timestamp":1634009093252,"user_tz":-120,"elapsed":96,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"2604197b-4bc8-4eef-dbc5-9d786e05e343"},"source":["x = np.random.randint(low=0, high=len(train_set_custom))                        # Pick a random datapoint\n","\n","sample = train_set_custom[x]\n","input, labels = sample \n","\n","image, random_number = input \n","label_image, label_sum = labels\n","\n","plt.imshow(image.squeeze())                                                     # Plot 2-d image tensor\n","plt.title(\"Label image \" + str(label_image.numpy()) + \" Random number \" + str(categorical_to_num(random_number.numpy())) + \" Sum \" + str(label_sum.numpy()))\n","plt.show()"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"UORBwArSqTvg"},"source":["# Network"]},{"cell_type":"code","metadata":{"id":"4ZURxNFmhz4d","executionInfo":{"status":"ok","timestamp":1634009093253,"user_tz":-120,"elapsed":79,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"rj_yAbCfgBUD","executionInfo":{"status":"ok","timestamp":1634009093255,"user_tz":-120,"elapsed":69,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["class Network(nn.Module):\n","  \"\"\"\n","  Network class that inherits nn.Module\n","  Methods: \n","    forward: forward pass the layers of the network to return output\n","  \"\"\"\n","\n","  def __init__(self):\n","    \n","    super().__init__()\n","\n","    # In comments we will be writing the input (s, s, channels), kernel (s, s, channels, no. of kernels), \n","    # output (x, y, channels), receptive field (s, s, channels)\n","\n","    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)         # (28, 28, 1)   --> (3, 3, 1, 32)     --> (28, 28, 32)  [R.F. (3, 3, 1)]\n","    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)        # (28, 28, 32)  --> (3, 3, 32, 64)    --> (28, 28, 64)  [R.F. (5, 5, 32)]\n","    self.pool1 = nn.MaxPool2d(2, 2)                     # (28, 28, 64)  --> (2, 2)            --> (14, 14, 64)  [R.F. (10, 10, 64)]\n","    \n","    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)       # (14, 14, 64)  --> (3, 3, 64, 128)   --> (14, 14, 128) [R.F. (12, 12, 64)]\n","    self.conv4 = nn.Conv2d(128, 256, 3, padding=1)      # (14, 14, 128) --> (3, 3, 128, 256)  --> (14, 14, 256) [R.F. (14, 14, 128)]\n","    self.pool2 = nn.MaxPool2d(2, 2)                     # (14, 14, 256) --> (2, 2)            --> (7, 7, 256)   [R.F. (28, 28, 256)]\n","    \n","    self.conv5 = nn.Conv2d(256, 512, 3)                 # (7, 7, 256)   --> (3, 3, 256, 512)  --> (5, 5, 512)   [R.F. (30, 30, 256)]\n","    self.conv6 = nn.Conv2d(512, 1024, 3)                # (5, 5, 512)   --> (3, 3, 512, 1024) --> (3, 3, 1024)  [R.F. (32, 32, 512)]\n","    self.conv7 = nn.Conv2d(1024, 10, 3)                 # (3, 3, 1024)  --> (3, 3, 1024, 10)  --> (1, 1, 10)    [R.F. (34, 34, 1024)]\n","    \n","    self.fc1 = nn.Linear(in_features=20, out_features=100)      # We add a large number of neurons at this stage for the network to understand addition\n","    self.fc2 = nn.Linear(in_features=100, out_features=50)      # FC layer\n","\n","    self.out1 = nn.Linear(in_features=50, out_features=10)      # Output in 1-hot encoding for the image label\n","    self.out2 = nn.Linear(in_features=50, out_features=19)      # Output in 1-hot encoding for the sum\n","\n","\n","  def forward(self, x, random_num):\n","\n","    \"\"\"\n","    Forward pass for the network\n","    Inputs: \n","      x: Image \n","      random_num: 1-hot encoding of a random number \n","    Outputs: \n","      x1: 1-hot encoding of prediction of image number \n","      x2: 1-hot encoding of prediction of sum \n","    \"\"\"\n","\n","    # first block\n","    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n","\n","    # second block\n","    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n","\n","    # third block \n","    x = self.conv7(F.relu(self.conv6(F.relu(self.conv5(x)))))\n","\n","    # FC \n","    x, random_num = x.reshape(-1, 10), random_num.reshape(-1, 10)               # At this layer, we concantenate the two inputs\n","    x = torch.cat((x, random_num), dim=1)\n","    \n","    x = self.fc2(F.relu(self.fc1(x)))\n","\n","    x1, x2 = self.out1(x), self.out2(x)\n","    x1, x2 = x1.view(-1, 10), x2.view(-1, 19)\n","\n","    return F.log_softmax(x1, dim=0), F.log_softmax(x2, dim=0)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVFDxj0xmh2H","executionInfo":{"status":"ok","timestamp":1634009107853,"user_tz":-120,"elapsed":14659,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["# Create an instance of our network and use it on device \n","model = Network().to(device)                                  "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLoTKqVN-aLk","executionInfo":{"status":"ok","timestamp":1634009107858,"user_tz":-120,"elapsed":30,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"8565bdc3-0ebd-445b-ae4b-3964b7b324bd"},"source":["# printing all named parameters in the network\n","for name, param in model.named_parameters():\n","  print(name, '\\t\\t', param.shape) "],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["conv1.weight \t\t torch.Size([32, 1, 3, 3])\n","conv1.bias \t\t torch.Size([32])\n","conv2.weight \t\t torch.Size([64, 32, 3, 3])\n","conv2.bias \t\t torch.Size([64])\n","conv3.weight \t\t torch.Size([128, 64, 3, 3])\n","conv3.bias \t\t torch.Size([128])\n","conv4.weight \t\t torch.Size([256, 128, 3, 3])\n","conv4.bias \t\t torch.Size([256])\n","conv5.weight \t\t torch.Size([512, 256, 3, 3])\n","conv5.bias \t\t torch.Size([512])\n","conv6.weight \t\t torch.Size([1024, 512, 3, 3])\n","conv6.bias \t\t torch.Size([1024])\n","conv7.weight \t\t torch.Size([10, 1024, 3, 3])\n","conv7.bias \t\t torch.Size([10])\n","fc1.weight \t\t torch.Size([100, 20])\n","fc1.bias \t\t torch.Size([100])\n","fc2.weight \t\t torch.Size([50, 100])\n","fc2.bias \t\t torch.Size([50])\n","out1.weight \t\t torch.Size([10, 50])\n","out1.bias \t\t torch.Size([10])\n","out2.weight \t\t torch.Size([19, 50])\n","out2.bias \t\t torch.Size([19])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"495uioRRfdny","executionInfo":{"status":"ok","timestamp":1634009108529,"user_tz":-120,"elapsed":679,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"33ff719e-dfe0-40a5-a4ca-8f972fd8af1c"},"source":["from torchsummary import summary\n","\n","# show model summary (layers, output shapes and number of paramaters)\n","summary(model, [(1, 28, 28), (1, 1, 10)]) "],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","            Conv2d-2           [-1, 64, 28, 28]          18,496\n","         MaxPool2d-3           [-1, 64, 14, 14]               0\n","            Conv2d-4          [-1, 128, 14, 14]          73,856\n","            Conv2d-5          [-1, 256, 14, 14]         295,168\n","         MaxPool2d-6            [-1, 256, 7, 7]               0\n","            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n","            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n","            Conv2d-9             [-1, 10, 1, 1]          92,170\n","           Linear-10                  [-1, 100]           2,100\n","           Linear-11                   [-1, 50]           5,050\n","           Linear-12                   [-1, 10]             510\n","           Linear-13                   [-1, 19]             969\n","================================================================\n","Total params: 6,388,415\n","Trainable params: 6,388,415\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 1.51\n","Params size (MB): 24.37\n","Estimated Total Size (MB): 25.91\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"markdown","metadata":{"id":"u1rc83wMqYnb"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"jHZijudRM_nW","executionInfo":{"status":"ok","timestamp":1634009108533,"user_tz":-120,"elapsed":50,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["def get_num_correct(preds, labels):\n","\n","  \"\"\"\n","  Returns number of correct predictions given \n","  Inputs: \n","    preds: tensor of predictions\n","    labels: tensor of labels\n","  \"\"\"\n","\n","  a = preds.argmax(dim=1)\n","  b = labels \n","\n","  return torch.sum((a==b)).item()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKtogr5Akeny","executionInfo":{"status":"ok","timestamp":1634009108535,"user_tz":-120,"elapsed":49,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}}},"source":["# Import tqdm for creating progress bar\n","from tqdm import tqdm\n","\n","def train(model, device, train_loader, optimizer):\n","\n","    model.train()                                                               # Tells model it's training\n","    pbar = tqdm(train_loader)\n","\n","    for batch_idx, (data, labels) in enumerate(pbar):                           # for loop for batches\n","\n","      images, random_numbers = data \n","      image_labels, sum_labels = labels \n","\n","      images, random_numbers = images.to(device), random_numbers.to(device)     # send data to device\n","      image_labels, sum_labels = image_labels.to(device), sum_labels.to(device) # send labels to device\n","\n","      optimizer.zero_grad()                                                     # make gradients zero \n","\n","      output = model(images, random_numbers)                                    # forward pass once to get outputs\n","      preds_images, preds_sums = output   \n","\n","      loss1 = F.cross_entropy(preds_images, image_labels)                       # get crossentropy loss on image labels\n","      loss2 = F.cross_entropy(preds_sums, sum_labels)                           # get crossentropy loss of sum labels\n","      loss = loss1 + loss2 \n","      loss.backward()                                                           # back propagation \n","\n","      optimizer.step()                                                          # change weights of the network\n","      \n","      pbar.set_description(desc= f'Train loss={loss.item()} batch_id={batch_idx}')\n","\n","\n","def test(model, device, test_loader):\n","  \n","  model.eval()                                                                  # Tells model it's evaluating\n","\n","  test_loss = 0                                                                 # initalize test loss, # of correct labels = 0\n","  correct_image_label, correct_sum_label = 0, 0\n","  \n","  with torch.no_grad():                                                         # Without any change in gradients\n","    \n","    for data, labels in test_loader:\n","      \n","      images, random_numbers = data \n","      image_labels, sum_labels = labels \n","\n","      images, random_numbers = images.to(device), random_numbers.to(device)     # send data to device\n","      image_labels, sum_labels = image_labels.to(device), sum_labels.to(device) # send labels to device\n","\n","      output = model(images, random_numbers)                                    # forward pass once to get outputs                             \n","      preds_images, preds_sums = output\n","   \n","      loss1 = F.cross_entropy(preds_images, image_labels) \n","      loss2 = F.cross_entropy(preds_sums, sum_labels)\n","      loss = loss1 + loss2 \n","\n","      test_loss += loss.item()                                                  # Calculate test loss\n","        \n","      correct_image_label += get_num_correct(preds_images, image_labels)        # Get number of correct predictions \n","      correct_sum_label += get_num_correct(preds_sums, sum_labels)              # for images and sum\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy image labels: {}/{} ({:.0f}%), Accuracy sum labels: {}/{} ({:.0f}%)'.format(\n","        test_loss / len(test_loader.dataset), correct_image_label, len(test_loader.dataset),\n","        100. * correct_image_label / len(test_loader.dataset), correct_sum_label, len(test_loader.dataset),\n","        100. * correct_sum_label / len(test_loader.dataset)))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYXtYU8YkuDP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634009366448,"user_tz":-120,"elapsed":257960,"user":{"displayName":"Lavanya Nemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gju3SlZQ0duQdpEWDk86a-6AS3SfaGv_H9FWpam=s64","userId":"17104860808699095583"}},"outputId":"550052c3-fd18-4956-d626-68e248970a39"},"source":["optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)                # Set optimizer to Stocastic gradient descent\n","\n","for epoch in range(0, 5):                                                       # for loop for each epoch of training \n","    train(model, device, train_loader, optimizer)                               # train\n","    test(model, device, test_loader)                                            # test"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Train loss=2.001474142074585 batch_id=599: 100%|██████████| 600/600 [00:46<00:00, 12.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0199, Accuracy image labels: 9653/10000 (97%), Accuracy sum labels: 2631/10000 (26%)\n"]},{"output_type":"stream","name":"stderr","text":["Train loss=1.315924048423767 batch_id=599: 100%|██████████| 600/600 [00:47<00:00, 12.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0120, Accuracy image labels: 9873/10000 (99%), Accuracy sum labels: 6109/10000 (61%)\n"]},{"output_type":"stream","name":"stderr","text":["Train loss=0.335256427526474 batch_id=599: 100%|██████████| 600/600 [00:47<00:00, 12.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0048, Accuracy image labels: 9920/10000 (99%), Accuracy sum labels: 8989/10000 (90%)\n"]},{"output_type":"stream","name":"stderr","text":["Train loss=0.31646761298179626 batch_id=599: 100%|██████████| 600/600 [00:46<00:00, 12.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0021, Accuracy image labels: 9914/10000 (99%), Accuracy sum labels: 9616/10000 (96%)\n"]},{"output_type":"stream","name":"stderr","text":["Train loss=0.10507485270500183 batch_id=599: 100%|██████████| 600/600 [00:46<00:00, 12.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0012, Accuracy image labels: 9928/10000 (99%), Accuracy sum labels: 9738/10000 (97%)\n"]}]}]}